{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-26 05:55:35,045\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "INFO:root:Try init sf in SIMULATION mode\n",
      "/usr/local/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-12-26 05:55:39,657\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=1.95gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-12-26 05:55:39,819\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-12-26 05:57:39,797 E 1577 1577] (raylet) node_manager.cc:3024: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: fa3159785f4e2f0f601fcd556465788de63e5560b3781b7a995484e5, IP: 172.21.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-26 05:58:39,804 E 1577 1577] (raylet) node_manager.cc:3024: 11 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: fa3159785f4e2f0f601fcd556465788de63e5560b3781b7a995484e5, IP: 172.21.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.21.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import secretflow as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob, carol = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('carol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CIFAR-10 dataset...\n",
      "Download completed.\n",
      "Extracting CIFAR-10 dataset...\n",
      "Extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# CIFAR-10 下载 URL\n",
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "dataset_dir = './cifar10_data'  # 保存数据的目录\n",
    "filename = 'cifar-10-python.tar.gz'\n",
    "\n",
    "# 下载文件（如果尚未下载）\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "file_path = os.path.join(dataset_dir, filename)\n",
    "\n",
    "# 如果文件不存在，下载文件\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading CIFAR-10 dataset...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Download completed.\")\n",
    "\n",
    "# 解压文件\n",
    "if not os.path.exists(os.path.join(dataset_dir, 'cifar-10-batches-py')):\n",
    "    print(\"Extracting CIFAR-10 dataset...\")\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=dataset_dir)\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "# 加载 CIFAR-10 数据集\n",
    "def load_cifar10_batch(batch_filename):\n",
    "    \"\"\"加载 CIFAR-10 数据批次\"\"\"\n",
    "    import pickle\n",
    "    with open(batch_filename, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='latin1')  # 注意 encoding='latin1'，以避免 Python 3 中的编码问题\n",
    "    return batch\n",
    "\n",
    "# 加载所有数据批次\n",
    "def load_cifar10_data(dataset_dir):\n",
    "    # 数据集文件路径\n",
    "    batches = []\n",
    "    for i in range(1, 6):  # CIFAR-10 数据集有 5 个批次\n",
    "        batch_filename = os.path.join(dataset_dir, 'cifar-10-batches-py', f'data_batch_{i}')\n",
    "        batches.append(load_cifar10_batch(batch_filename))\n",
    "\n",
    "    # 合并所有批次的数据\n",
    "    images = np.concatenate([batch['data'] for batch in batches], axis=0)\n",
    "    labels = np.concatenate([batch['labels'] for batch in batches], axis=0)\n",
    "    \n",
    "    # 转换图片数据形状 (N, 3, 32, 32)\n",
    "    images = images.reshape(-1, 3, 32, 32).astype(np.uint8)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = load_cifar10_data(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_images = image[:15000]\n",
    "bob_images = image[15000:35000]\n",
    "carol_images = image[35000:]\n",
    "\n",
    "alice_labels = label[:15000]\n",
    "bob_labels = label[15000:35000]\n",
    "carol_labels = label[35000:]\n",
    "\n",
    "alice_partition_images = alice(lambda x: x)(alice_images)\n",
    "bob_partition_images = bob(lambda x: x)(bob_images)\n",
    "carol_partition_images = carol(lambda x: x)(carol_images)\n",
    "\n",
    "alice_partition_labels = alice(lambda x: x)(alice_labels)\n",
    "bob_partition_labels = bob(lambda x: x)(bob_labels)\n",
    "carol_partition_labels = carol(lambda x: x)(carol_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建联邦数据集 FedNdarray\n",
    "from secretflow.data.ndarray import FedNdarray, PartitionWay\n",
    "\n",
    "# 图像数据\n",
    "federated_images = FedNdarray(\n",
    "    partitions={alice: alice_partition_images, bob: bob_partition_images, carol: carol_partition_images},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# 标签数据\n",
    "federated_labels = FedNdarray(\n",
    "    partitions={alice: alice_partition_labels, bob: bob_partition_labels, carol: carol_partition_labels},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# # 检查分区信息\n",
    "print(\"Image partitions shape:\", federated_images.partition_shape())\n",
    "print(\"Label partitions shape:\", federated_labels.partition_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_dense_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(28, 28, 1)),  # 输入形状为 28x28 的灰度图像，包含 1 个通道\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 1\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 2\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层\n",
    "            layers.Flatten(),  # 拉平为一维向量\n",
    "            layers.Dense(128, activation=\"relu\"),  # 全连接层\n",
    "            layers.Dense(10, activation=\"softmax\"),  # 输出层，10 个类别\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 在外部创建共享模型\n",
    "shared_model = create_dense_model()\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer.build(shared_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 on Alice's partition...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.21.0.2, ID: fa3159785f4e2f0f601fcd556465788de63e5560b3781b7a995484e5) where the task (task ID: 32a94ac0b09047fd8e1ffe7adb89aa4bb8e618f001000000, name=pyu_fn, pid=1659, memory used=0.04GB) was running was 7.37GB / 7.75GB (0.950688), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a13c0e7b189b786aa91ea1bf5beda710eac74b2c98daa048c1188f0a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.21.0.2`. To see the logs of the worker, use `ray logs worker-a13c0e7b189b786aa91ea1bf5beda710eac74b2c98daa048c1188f0a*out -ip 172.21.0.2. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n385\t1.03\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node /root/....\n1457\t0.67\t/usr/local/bin/python -m ipykernel_launcher --f=/root/.local/share/jupyter/runtime/kernel-v329760dd8...\n167\t0.16\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node --dns-r...\n1496\t0.06\t/usr/local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n1545\t0.05\t/usr/local/bin/python /usr/local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=127....\n135\t0.05\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node /root/....\n1637\t0.04\t/usr/local/bin/python -u /usr/local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-ad...\n1577\t0.04\t/usr/local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray...\n1659\t0.04\tray::IDLE\n178\t0.04\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node /root/....\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m     shared_model\u001b[38;5;241m.\u001b[39mset_weights(weights_share)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on Alice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m weights_alice, loss_alice, acc_alice, avg_gradients_alice \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43malice_partition_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malice_partition_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_weights_alice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss on Alice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_alice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy on Alice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_alice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on Bob\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(partition_data, partition_labels, shared_model, previous_weights, optimizer, batch_size, mu, temperature)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_one_epoch\u001b[39m(partition_data, partition_labels, shared_model, previous_weights, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 从分区中提取数据和标签\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mreveal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     labels \u001b[38;5;241m=\u001b[39m reveal(partition_labels)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 调整数据形状为模型的输入格式\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/secretflow/device/driver.py:161\u001b[0m, in \u001b[0;36mreveal\u001b[0;34m(func_or_object, heu_encoder)\u001b[0m\n\u001b[1;32m    158\u001b[0m         logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting teeu data from TEEU \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mparty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    160\u001b[0m cur_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 161\u001b[0m all_object \u001b[38;5;241m=\u001b[39m \u001b[43msfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_object_refs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m new_flatten_val \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flatten_val:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/secretflow/distributed/primitive.py:164\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rayfed\u001b[38;5;241m.\u001b[39mget(object_refs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_distribution_mode() \u001b[38;5;241m==\u001b[39m DISTRIBUTION_MODE\u001b[38;5;241m.\u001b[39mSIMULATION:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_distribution_mode() \u001b[38;5;241m==\u001b[39m DISTRIBUTION_MODE\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m object_refs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:22\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ray/_private/worker.py:2626\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2624\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2626\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   2629\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.21.0.2, ID: fa3159785f4e2f0f601fcd556465788de63e5560b3781b7a995484e5) where the task (task ID: 32a94ac0b09047fd8e1ffe7adb89aa4bb8e618f001000000, name=pyu_fn, pid=1659, memory used=0.04GB) was running was 7.37GB / 7.75GB (0.950688), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a13c0e7b189b786aa91ea1bf5beda710eac74b2c98daa048c1188f0a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.21.0.2`. To see the logs of the worker, use `ray logs worker-a13c0e7b189b786aa91ea1bf5beda710eac74b2c98daa048c1188f0a*out -ip 172.21.0.2. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n385\t1.03\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node /root/....\n1457\t0.67\t/usr/local/bin/python -m ipykernel_launcher --f=/root/.local/share/jupyter/runtime/kernel-v329760dd8...\n167\t0.16\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node --dns-r...\n1496\t0.06\t/usr/local/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2...\n1545\t0.05\t/usr/local/bin/python /usr/local/lib/python3.10/site-packages/ray/dashboard/dashboard.py --host=127....\n135\t0.05\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node /root/....\n1637\t0.04\t/usr/local/bin/python -u /usr/local/lib/python3.10/site-packages/ray/dashboard/agent.py --node-ip-ad...\n1577\t0.04\t/usr/local/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray...\n1659\t0.04\tray::IDLE\n178\t0.04\t/root/.vscode-server/cli/servers/Stable-fabdb6a30b49f79a7aba0f2ad9df9b399473380f/server/node /root/....\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from secretflow import reveal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 使用 Cosine Similarity 计算损失\n",
    "def cosine_similarity(a, b):\n",
    "    # 计算余弦相似度\n",
    "    dot_product = tf.reduce_sum(a * b, axis=-1)  # 点积\n",
    "    norm_a = tf.norm(a, axis=-1)  # a 的范数\n",
    "    norm_b = tf.norm(b, axis=-1)  # b 的范数\n",
    "    cosine_sim = dot_product / (norm_a * norm_b + 1e-8)  # 计算余弦相似度，防止除以零\n",
    "    return cosine_sim\n",
    "\n",
    "# 定义单轮训练函数（手动计算损失和梯度）\n",
    "def train_one_epoch(partition_data, partition_labels, shared_model, previous_weights, optimizer=None, batch_size=128, mu = 2, temperature = 0.5):\n",
    "    # 从分区中提取数据和标签\n",
    "    data = reveal(partition_data)\n",
    "    labels = reveal(partition_labels)\n",
    "    \n",
    "    # 调整数据形状为模型的输入格式\n",
    "    data = data.reshape(-1, 28, 28, 1)  # 确保形状为 (样本数, 28, 28, 1)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = create_dense_model()\n",
    "    previous_models = [create_dense_model() for _ in previous_weights]\n",
    "\n",
    "\n",
    "    \n",
    "    # 初始化权重（如果提供了初始权重）\n",
    "    if shared_model.get_weights() is not None:\n",
    "        model.set_weights(shared_model.get_weights())\n",
    "    for previous_model, previous_weight in zip(previous_models, previous_weights):\n",
    "        if previous_weight is not None:  # 检查每个权重是否为 None\n",
    "            previous_model.set_weights(previous_weight)\n",
    "\n",
    "    # 使用优化器（默认是 Adam，如果没有传入）\n",
    "    if optimizer is None:\n",
    "        optimizer = tf.keras.optimizers.Adam()  # 默认使用 Adam 优化器\n",
    "    \n",
    "    # 手动计算损失\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(batch_size)  # 使用数据集和批次大小\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # 创建准确率计算指标\n",
    "\n",
    "    accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "    \n",
    "    for batch_data, batch_labels in tqdm(dataset, desc=\"Training Progress\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向传播\n",
    "            predictions = model(batch_data, training=True)\n",
    "            predictions_shared = shared_model(batch_data, training=True)\n",
    "            predictions_previous_models = [model(batch_data, training=True) for model in previous_models]\n",
    "\n",
    "            # 计算余弦相似度并生成新的预测结果\n",
    "            new_predictions = cosine_similarity(predictions, predictions_shared)\n",
    "            new_predictions_previous = [cosine_similarity(predictions, predictions_previous_model) for predictions_previous_model in predictions_previous_models]\n",
    "\n",
    "            # 变形为 (batch_size, 1)\n",
    "            logits = tf.reshape(new_predictions, (-1, 1))\n",
    "            logits_previous = [tf.reshape(new_prediction_previous, (-1, 1)) for new_prediction_previous in new_predictions_previous]\n",
    "            logits_previous_cat = tf.concat(logits_previous, axis=1)\n",
    "            logits = tf.concat([logits, logits_previous_cat], axis=1)\n",
    "            logits /= temperature\n",
    "\n",
    "            # 计算损失\n",
    "            loss1 = tf.keras.losses.sparse_categorical_crossentropy(batch_labels, predictions, from_logits=False)\n",
    "            loss1 = tf.reduce_mean(loss1)  # 取平均值\n",
    "\n",
    "            # 计算对比损失 (第二部分)\n",
    "            # 假设使用与目标相同的标签，可以根据需要修改\n",
    "            labels = tf.zeros_like(batch_labels, dtype=tf.int64)\n",
    "            loss2 = mu * tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=False))\n",
    "\n",
    "            # 总损失 = 交叉熵损失 + 对比损失\n",
    "            loss = loss1 + loss2\n",
    "        \n",
    "        # 计算梯度\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # 应用梯度更新权重\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        epoch_loss += loss.numpy()  # 累积每批次的损失\n",
    "        \n",
    "        # 更新准确率\n",
    "        accuracy_metric.update_state(batch_labels, predictions)\n",
    "    \n",
    "    # 计算整个 epoch 的平均损失和准确率\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    avg_accuracy = accuracy_metric.result().numpy()  # 获取当前的准确率\n",
    "    accuracy_metric.reset_states()  # 重置准确率计算器\n",
    "    avg_gradients = [grad / len(dataset) for grad in accumulated_gradients]  # 平均每个梯度\n",
    "    \n",
    "    return model.get_weights(), avg_loss, avg_accuracy, avg_gradients  # 返回当前损失、准确率和更新后的权重\n",
    "\n",
    "# 外部控制 epochs 的循环\n",
    "num_epochs = 10\n",
    "weights_share = None\n",
    "weights_alice = None  # 初始权重为空\n",
    "weights_bob = None\n",
    "weights_carol = None\n",
    "previous_weights_alice = [None, None]\n",
    "previous_weights_bob = [None, None]\n",
    "previous_weights_carol = [None, None]\n",
    "\n",
    "# 假设 Alice 和 Bob 的数据量\n",
    "alice_data_size = len(alice_images)\n",
    "bob_data_size = len(bob_images)\n",
    "carol_data_size = len(carol_images)\n",
    "\n",
    "# 计算总数据量\n",
    "total_data_size = alice_data_size + bob_data_size + carol_data_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 更新共享模型权重的加权平均\n",
    "    if weights_alice is not None and weights_bob is not None and weights_carol is not None:\n",
    "        # 加权平均\n",
    "        weights_share = [\n",
    "            (wa * alice_data_size + wb * bob_data_size + wc * carol_data_size) / total_data_size\n",
    "            for wa, wb, wc in zip(weights_alice, weights_bob, weights_carol)\n",
    "        ]\n",
    "        # 设置共享模型的权重\n",
    "        shared_model.set_weights(weights_share)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Alice's partition...\")\n",
    "    weights_alice, loss_alice, acc_alice, avg_gradients_alice = train_one_epoch(alice_partition_images, alice_partition_labels, shared_model, previous_weights_alice)\n",
    "    print(f\"Loss on Alice's partition: {loss_alice}, Accuracy on Alice's partition: {acc_alice}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Bob's partition...\")\n",
    "    weights_bob, loss_bob, acc_bob, avg_gradients_bob = train_one_epoch(bob_partition_images, bob_partition_labels, shared_model, previous_weights_bob)\n",
    "    print(f\"Loss on Bob's partition: {loss_bob}, Accuracy on Bob's partition: {acc_bob}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Carol's partition...\")\n",
    "    weights_carol, loss_carol, acc_carol, avg_gradients_carol = train_one_epoch(carol_partition_images, carol_partition_labels, shared_model, previous_weights_carol)\n",
    "    print(f\"Loss on Carol's partition: {loss_carol}, Accuracy on Carol's partition: {acc_carol}\")\n",
    "\n",
    "    # 保存当前权重作为下一轮的 \"previous_weights\"\n",
    "    previous_weights_alice = [weights_bob, weights_carol]\n",
    "    previous_weights_bob = [weights_alice, weights_carol]\n",
    "    previous_weights_carol= [weights_alice, weights_bob]\n",
    "\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
