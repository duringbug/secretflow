{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-26 09:50:50,231\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "INFO:root:Try init sf in SIMULATION mode\n",
      "/usr/local/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-12-26 09:50:53,469\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.92gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-12-26 09:50:53,610\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import secretflow as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob, carol = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('carol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# CIFAR-10 下载 URL\n",
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "dataset_dir = './cifar10_data'  # 保存数据的目录\n",
    "filename = 'cifar-10-python.tar.gz'\n",
    "\n",
    "# 下载文件（如果尚未下载）\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "file_path = os.path.join(dataset_dir, filename)\n",
    "\n",
    "# 如果文件不存在，下载文件\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading CIFAR-10 dataset...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Download completed.\")\n",
    "\n",
    "# 解压文件\n",
    "if not os.path.exists(os.path.join(dataset_dir, 'cifar-10-batches-py')):\n",
    "    print(\"Extracting CIFAR-10 dataset...\")\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=dataset_dir)\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "# 加载 CIFAR-10 数据集\n",
    "def load_cifar10_batch(batch_filename):\n",
    "    \"\"\"加载 CIFAR-10 数据批次\"\"\"\n",
    "    import pickle\n",
    "    with open(batch_filename, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='latin1')  # 注意 encoding='latin1'，以避免 Python 3 中的编码问题\n",
    "    return batch\n",
    "\n",
    "# 加载所有数据批次\n",
    "def load_cifar10_data(dataset_dir):\n",
    "    # 数据集文件路径\n",
    "    batches = []\n",
    "    for i in range(1, 6):  # CIFAR-10 数据集有 5 个批次\n",
    "        batch_filename = os.path.join(dataset_dir, 'cifar-10-batches-py', f'data_batch_{i}')\n",
    "        batches.append(load_cifar10_batch(batch_filename))\n",
    "\n",
    "    # 合并所有批次的数据\n",
    "    images = np.concatenate([batch['data'] for batch in batches], axis=0)\n",
    "    labels = np.concatenate([batch['labels'] for batch in batches], axis=0)\n",
    "    \n",
    "    # 转换图片数据形状 (N, 3, 32, 32)\n",
    "    images = images.reshape(-1, 3, 32, 32).astype(np.uint8)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = load_cifar10_data(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_images = image[:15000]\n",
    "bob_images = image[15000:35000]\n",
    "carol_images = image[35000:]\n",
    "\n",
    "alice_labels = label[:15000]\n",
    "bob_labels = label[15000:35000]\n",
    "carol_labels = label[35000:]\n",
    "\n",
    "alice_partition_images = alice(lambda x: x)(alice_images)\n",
    "bob_partition_images = bob(lambda x: x)(bob_images)\n",
    "carol_partition_images = carol(lambda x: x)(carol_images)\n",
    "\n",
    "alice_partition_labels = alice(lambda x: x)(alice_labels)\n",
    "bob_partition_labels = bob(lambda x: x)(bob_labels)\n",
    "carol_partition_labels = carol(lambda x: x)(carol_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image partitions shape: {PYURuntime(alice): (15000, 3, 32, 32), PYURuntime(bob): (20000, 3, 32, 32), PYURuntime(carol): (15000, 3, 32, 32)}\n",
      "Label partitions shape: {PYURuntime(alice): (15000,), PYURuntime(bob): (20000,), PYURuntime(carol): (15000,)}\n"
     ]
    }
   ],
   "source": [
    "# 创建联邦数据集 FedNdarray\n",
    "from secretflow.data.ndarray import FedNdarray, PartitionWay\n",
    "\n",
    "# 图像数据\n",
    "federated_images = FedNdarray(\n",
    "    partitions={alice: alice_partition_images, bob: bob_partition_images, carol: carol_partition_images},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# 标签数据\n",
    "federated_labels = FedNdarray(\n",
    "    partitions={alice: alice_partition_labels, bob: bob_partition_labels, carol: carol_partition_labels},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# # 检查分区信息\n",
    "print(\"Image partitions shape:\", federated_images.partition_shape())\n",
    "print(\"Label partitions shape:\", federated_labels.partition_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 09:50:59.501370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_dense_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(32, 32, 3)),  # 输入形状为 32*32 的 RGB 图像\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 1\n",
    "            layers.BatchNormalization(),  # 批归一化\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层 1\n",
    "            \n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 2\n",
    "            layers.BatchNormalization(),  # 批归一化\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层 2\n",
    "            \n",
    "            layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 3\n",
    "            layers.BatchNormalization(),  # 批归一化\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层 3\n",
    "            \n",
    "            layers.Flatten(),  # 拉平为一维向量\n",
    "            layers.Dense(512, activation=\"relu\"),  # 全连接层 1\n",
    "            layers.Dropout(0.5),  # Dropout 层，防止过拟合\n",
    "            layers.Dense(10, activation=\"softmax\")  # 输出层，10 个类别\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 在外部创建共享模型\n",
    "shared_model = create_dense_model()\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer.build(shared_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:26<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 4.267078446129621, Accuracy on Alice's partition: 0.25866666436195374\n",
      "Epoch 1/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:35<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 4.130200509053127, Accuracy on Bob's partition: 0.2722499966621399\n",
      "Epoch 1/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:27<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 4.180807222754268, Accuracy on Carol's partition: 0.2552666664123535\n",
      "Epoch 2/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:26<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 3.8735493886268744, Accuracy on Alice's partition: 0.3973333239555359\n",
      "Epoch 2/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:35<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 3.848094396530443, Accuracy on Bob's partition: 0.39640000462532043\n",
      "Epoch 2/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:26<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 3.8828020095825195, Accuracy on Carol's partition: 0.382999986410141\n",
      "Epoch 3/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:26<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 3.7279583660222717, Accuracy on Alice's partition: 0.44473332166671753\n",
      "Epoch 3/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:36<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 3.714131212538215, Accuracy on Bob's partition: 0.4503999948501587\n",
      "Epoch 3/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:26<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 3.7238851240125754, Accuracy on Carol's partition: 0.4528000056743622\n",
      "Epoch 4/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:26<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 3.620473344447249, Accuracy on Alice's partition: 0.48506665229797363\n",
      "Epoch 4/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:35<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 3.5990986641804885, Accuracy on Bob's partition: 0.49184998869895935\n",
      "Epoch 4/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:29<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 3.6269524784411415, Accuracy on Carol's partition: 0.4844000041484833\n",
      "Epoch 5/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:28<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 3.5310612755306696, Accuracy on Alice's partition: 0.5208666920661926\n",
      "Epoch 5/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 27/157 [00:06<00:31,  4.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss on Alice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_alice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy on Alice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_alice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on Bob\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m weights_bob, loss_bob, acc_bob, avg_gradients_bob \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbob_partition_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbob_partition_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_weights_bob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss on Bob\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_bob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy on Bob\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_bob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on Carol\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms partition...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(partition_data, partition_labels, shared_model, previous_weights, optimizer, batch_size, mu, temperature)\u001b[0m\n\u001b[1;32m     53\u001b[0m predictions_previous_models \u001b[38;5;241m=\u001b[39m [model(batch_data, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m previous_models]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 计算余弦相似度并生成新的预测结果\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m new_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_shared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m new_predictions_previous \u001b[38;5;241m=\u001b[39m [cosine_similarity(predictions, predictions_previous_model) \u001b[38;5;28;01mfor\u001b[39;00m predictions_previous_model \u001b[38;5;129;01min\u001b[39;00m predictions_previous_models]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# 变形为 (batch_size, 1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      8\u001b[0m dot_product \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(a \u001b[38;5;241m*\u001b[39m b, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 点积\u001b[39;00m\n\u001b[1;32m      9\u001b[0m norm_a \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnorm(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# a 的范数\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m norm_b \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# b 的范数\u001b[39;00m\n\u001b[1;32m     11\u001b[0m cosine_sim \u001b[38;5;241m=\u001b[39m dot_product \u001b[38;5;241m/\u001b[39m (norm_a \u001b[38;5;241m*\u001b[39m norm_b \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)  \u001b[38;5;66;03m# 计算余弦相似度，防止除以零\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cosine_sim\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/linalg_ops.py:621\u001b[0m, in \u001b[0;36mnorm_v2\u001b[0;34m(tensor, ord, axis, keepdims, name)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinalg.norm\u001b[39m\u001b[38;5;124m'\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm_v2\u001b[39m(tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m             keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    563\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the norm of vectors, matrices, and tensors.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m  This function can compute several different vector norms (the 1-norm, the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m  @end_compatibility\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 621\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;28;43mord\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mord\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m              \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m              \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m              \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:576\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    569\u001b[0m       logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    570\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and will \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    571\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbe removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date),\n\u001b[1;32m    575\u001b[0m           instructions)\n\u001b[0;32m--> 576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/linalg_ops.py:722\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(tensor, ord, axis, keepdims, name, keep_dims)\u001b[0m\n\u001b[1;32m    717\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    718\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be None, an integer, or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuple of 2 unique integers, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    721\u001b[0m supported_vector_norms \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39minf]\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misreal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mord\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_vector_norms:\n\u001b[1;32m    723\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mord\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a supported vector norm, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mord\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36misreal\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from secretflow import reveal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 使用 Cosine Similarity 计算损失\n",
    "def cosine_similarity(a, b):\n",
    "    # 计算余弦相似度\n",
    "    dot_product = tf.reduce_sum(a * b, axis=-1)  # 点积\n",
    "    norm_a = tf.norm(a, axis=-1)  # a 的范数\n",
    "    norm_b = tf.norm(b, axis=-1)  # b 的范数\n",
    "    cosine_sim = dot_product / (norm_a * norm_b + 1e-8)  # 计算余弦相似度，防止除以零\n",
    "    return cosine_sim\n",
    "\n",
    "# 定义单轮训练函数（手动计算损失和梯度）\n",
    "def train_one_epoch(partition_data, partition_labels, shared_model, previous_weights, optimizer=None, batch_size=128, mu = 2, temperature = 0.5):\n",
    "    # 从分区中提取数据和标签\n",
    "    data = reveal(partition_data)\n",
    "    labels = reveal(partition_labels)\n",
    "    \n",
    "    # 调整数据形状为模型的输入格式\n",
    "    data = data.reshape(-1, 32, 32, 3)  # 确保形状为 (样本数, 28, 28, 1)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = create_dense_model()\n",
    "    previous_models = [create_dense_model() for _ in previous_weights]\n",
    "\n",
    "\n",
    "    \n",
    "    # 初始化权重（如果提供了初始权重）\n",
    "    if shared_model.get_weights() is not None:\n",
    "        model.set_weights(shared_model.get_weights())\n",
    "    for previous_model, previous_weight in zip(previous_models, previous_weights):\n",
    "        if previous_weight is not None:  # 检查每个权重是否为 None\n",
    "            previous_model.set_weights(previous_weight)\n",
    "\n",
    "    # 使用优化器（默认是 Adam，如果没有传入）\n",
    "    if optimizer is None:\n",
    "        optimizer = tf.keras.optimizers.Adam()  # 默认使用 Adam 优化器\n",
    "    \n",
    "    # 手动计算损失\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(batch_size)  # 使用数据集和批次大小\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # 创建准确率计算指标\n",
    "\n",
    "    accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "    \n",
    "    for batch_data, batch_labels in tqdm(dataset, desc=\"Training Progress\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向传播\n",
    "            predictions = model(batch_data, training=True)\n",
    "            predictions_shared = shared_model(batch_data, training=True)\n",
    "            predictions_previous_models = [model(batch_data, training=True) for model in previous_models]\n",
    "\n",
    "            # 计算余弦相似度并生成新的预测结果\n",
    "            new_predictions = cosine_similarity(predictions, predictions_shared)\n",
    "            new_predictions_previous = [cosine_similarity(predictions, predictions_previous_model) for predictions_previous_model in predictions_previous_models]\n",
    "\n",
    "            # 变形为 (batch_size, 1)\n",
    "            logits = tf.reshape(new_predictions, (-1, 1))\n",
    "            logits_previous = [tf.reshape(new_prediction_previous, (-1, 1)) for new_prediction_previous in new_predictions_previous]\n",
    "            logits_previous_cat = tf.concat(logits_previous, axis=1)\n",
    "            logits = tf.concat([logits, logits_previous_cat], axis=1)\n",
    "            logits /= temperature\n",
    "\n",
    "            # 计算损失\n",
    "            loss1 = tf.keras.losses.sparse_categorical_crossentropy(batch_labels, predictions, from_logits=False)\n",
    "            loss1 = tf.reduce_mean(loss1)  # 取平均值\n",
    "\n",
    "            # 计算对比损失 (第二部分)\n",
    "            # 假设使用与目标相同的标签，可以根据需要修改\n",
    "            labels = tf.zeros_like(batch_labels, dtype=tf.int64)\n",
    "            loss2 = mu * tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=False))\n",
    "\n",
    "            # 总损失 = 交叉熵损失 + 对比损失\n",
    "            loss = loss1 + loss2\n",
    "        \n",
    "        # 计算梯度\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # 应用梯度更新权重\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        epoch_loss += loss.numpy()  # 累积每批次的损失\n",
    "        \n",
    "        # 更新准确率\n",
    "        accuracy_metric.update_state(batch_labels, predictions)\n",
    "    \n",
    "    # 计算整个 epoch 的平均损失和准确率\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    avg_accuracy = accuracy_metric.result().numpy()  # 获取当前的准确率\n",
    "    accuracy_metric.reset_states()  # 重置准确率计算器\n",
    "    avg_gradients = [grad / len(dataset) for grad in accumulated_gradients]  # 平均每个梯度\n",
    "    \n",
    "    return model.get_weights(), avg_loss, avg_accuracy, avg_gradients  # 返回当前损失、准确率和更新后的权重\n",
    "\n",
    "# 外部控制 epochs 的循环\n",
    "num_epochs = 10\n",
    "weights_share = None\n",
    "weights_alice = None  # 初始权重为空\n",
    "weights_bob = None\n",
    "weights_carol = None\n",
    "previous_weights_alice = [None, None]\n",
    "previous_weights_bob = [None, None]\n",
    "previous_weights_carol = [None, None]\n",
    "\n",
    "# 假设 Alice 和 Bob 的数据量\n",
    "alice_data_size = len(alice_images)\n",
    "bob_data_size = len(bob_images)\n",
    "carol_data_size = len(carol_images)\n",
    "\n",
    "# 计算总数据量\n",
    "total_data_size = alice_data_size + bob_data_size + carol_data_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 更新共享模型权重的加权平均\n",
    "    if weights_alice is not None and weights_bob is not None and weights_carol is not None:\n",
    "        # 加权平均\n",
    "        weights_share = [\n",
    "            (wa * alice_data_size + wb * bob_data_size + wc * carol_data_size) / total_data_size\n",
    "            for wa, wb, wc in zip(weights_alice, weights_bob, weights_carol)\n",
    "        ]\n",
    "        # 设置共享模型的权重\n",
    "        shared_model.set_weights(weights_share)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Alice's partition...\")\n",
    "    weights_alice, loss_alice, acc_alice, avg_gradients_alice = train_one_epoch(alice_partition_images, alice_partition_labels, shared_model, previous_weights_alice)\n",
    "    print(f\"Loss on Alice's partition: {loss_alice}, Accuracy on Alice's partition: {acc_alice}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Bob's partition...\")\n",
    "    weights_bob, loss_bob, acc_bob, avg_gradients_bob = train_one_epoch(bob_partition_images, bob_partition_labels, shared_model, previous_weights_bob)\n",
    "    print(f\"Loss on Bob's partition: {loss_bob}, Accuracy on Bob's partition: {acc_bob}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Carol's partition...\")\n",
    "    weights_carol, loss_carol, acc_carol, avg_gradients_carol = train_one_epoch(carol_partition_images, carol_partition_labels, shared_model, previous_weights_carol)\n",
    "    print(f\"Loss on Carol's partition: {loss_carol}, Accuracy on Carol's partition: {acc_carol}\")\n",
    "\n",
    "    # 保存当前权重作为下一轮的 \"previous_weights\"\n",
    "    previous_weights_alice = [weights_bob, weights_carol]\n",
    "    previous_weights_bob = [weights_alice, weights_carol]\n",
    "    previous_weights_carol= [weights_alice, weights_bob]\n",
    "\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
