{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-16 09:50:33,584\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "INFO:root:Try init sf in SIMULATION mode\n",
      "/usr/local/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-12-16 09:50:36,700\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=1.66gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-12-16 09:50:37,894\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import secretflow as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob, carol = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('carol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from secretflow.utils.simulation.datasets import dataset\n",
    "\n",
    "mnist_dataset = dataset('mnist')\n",
    "mnist = np.load(mnist_dataset, allow_pickle=True)\n",
    "image = mnist['x_train']\n",
    "label = mnist['y_train']\n",
    "\n",
    "alice_data = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_images = image[:15000]\n",
    "bob_images = image[15000:35000]\n",
    "carol_images = image[35000:]\n",
    "\n",
    "alice_labels = label[:15000]\n",
    "bob_labels = label[15000:35000]\n",
    "carol_labels = label[35000:]\n",
    "\n",
    "alice_partition_images = alice(lambda x: x)(alice_images)\n",
    "bob_partition_images = bob(lambda x: x)(bob_images)\n",
    "carol_partition_images = carol(lambda x: x)(carol_images)\n",
    "\n",
    "alice_partition_labels = alice(lambda x: x)(alice_labels)\n",
    "bob_partition_labels = bob(lambda x: x)(bob_labels)\n",
    "carol_partition_labels = carol(lambda x: x)(carol_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image partitions shape: {PYURuntime(alice): (15000, 28, 28), PYURuntime(bob): (20000, 28, 28), PYURuntime(carol): (25000, 28, 28)}\n",
      "Label partitions shape: {PYURuntime(alice): (15000,), PYURuntime(bob): (20000,), PYURuntime(carol): (25000,)}\n"
     ]
    }
   ],
   "source": [
    "# 创建联邦数据集 FedNdarray\n",
    "from secretflow.data.ndarray import FedNdarray, PartitionWay\n",
    "\n",
    "# 图像数据\n",
    "federated_images = FedNdarray(\n",
    "    partitions={alice: alice_partition_images, bob: bob_partition_images, carol: carol_partition_images},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# 标签数据\n",
    "federated_labels = FedNdarray(\n",
    "    partitions={alice: alice_partition_labels, bob: bob_partition_labels, carol: carol_partition_labels},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# 检查分区信息\n",
    "print(\"Image partitions shape:\", federated_images.partition_shape())\n",
    "print(\"Label partitions shape:\", federated_labels.partition_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_dense_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(28, 28, 1)),  # 输入形状为 28x28 的灰度图像，包含 1 个通道\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 1\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 2\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层\n",
    "            layers.Flatten(),  # 拉平为一维向量\n",
    "            layers.Dense(128, activation=\"relu\"),  # 全连接层\n",
    "            layers.Dense(10, activation=\"softmax\"),  # 输出层，10 个类别\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 在外部创建共享模型\n",
    "shared_model = create_dense_model()\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer.build(shared_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:18<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 4.388160389358714, Accuracy on Alice's partition: 0.6768666505813599\n",
      "Epoch 1/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:22<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.8998531346108503, Accuracy on Bob's partition: 0.6974499821662903\n",
      "Epoch 1/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:30<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.940857909771861, Accuracy on Carol's partition: 0.7053200006484985\n",
      "Epoch 2/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:20<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.3722859661457902, Accuracy on Alice's partition: 0.9322666525840759\n",
      "Epoch 2/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:25<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.3854831677333563, Accuracy on Bob's partition: 0.9406499862670898\n",
      "Epoch 2/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:33<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.375627723275399, Accuracy on Carol's partition: 0.9408000111579895\n",
      "Epoch 3/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:20<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.300736174745075, Accuracy on Alice's partition: 0.9667333364486694\n",
      "Epoch 3/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:26<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.2845349084040163, Accuracy on Bob's partition: 0.9713500142097473\n",
      "Epoch 3/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:31<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.2817527882906856, Accuracy on Carol's partition: 0.9719600081443787\n",
      "Epoch 4/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.2576915146940846, Accuracy on Alice's partition: 0.9778000116348267\n",
      "Epoch 4/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:24<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.251405031058439, Accuracy on Bob's partition: 0.978600025177002\n",
      "Epoch 4/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:34<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.251981274205811, Accuracy on Carol's partition: 0.9811199903488159\n",
      "Epoch 5/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:19<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.2459058943441357, Accuracy on Alice's partition: 0.9815333485603333\n",
      "Epoch 5/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:25<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.2425613676666454, Accuracy on Bob's partition: 0.9834499955177307\n",
      "Epoch 5/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:31<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.244092212647808, Accuracy on Carol's partition: 0.9838799834251404\n",
      "Epoch 6/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:21<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.236182283546965, Accuracy on Alice's partition: 0.9845333099365234\n",
      "Epoch 6/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:25<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.234408425677354, Accuracy on Bob's partition: 0.986299991607666\n",
      "Epoch 6/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:35<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.2291393073237673, Accuracy on Carol's partition: 0.9863600134849548\n",
      "Epoch 7/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:20<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.2311988179966553, Accuracy on Alice's partition: 0.9864000082015991\n",
      "Epoch 7/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:27<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.231617789359609, Accuracy on Bob's partition: 0.9881500005722046\n",
      "Epoch 7/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:32<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.220428742924515, Accuracy on Carol's partition: 0.989359974861145\n",
      "Epoch 8/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:26<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.226062287718563, Accuracy on Alice's partition: 0.9872000217437744\n",
      "Epoch 8/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:29<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.2178759574890137, Accuracy on Bob's partition: 0.9909999966621399\n",
      "Epoch 8/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:32<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.2167779547827586, Accuracy on Carol's partition: 0.9904000163078308\n",
      "Epoch 9/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:18<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.2341840408616145, Accuracy on Alice's partition: 0.9886666536331177\n",
      "Epoch 9/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:25<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.228211916176377, Accuracy on Bob's partition: 0.9891499876976013\n",
      "Epoch 9/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:31<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.2258759457237867, Accuracy on Carol's partition: 0.990559995174408\n",
      "Epoch 10/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:19<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 2.228492516582295, Accuracy on Alice's partition: 0.9901999831199646\n",
      "Epoch 10/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:25<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 2.2192423966280215, Accuracy on Bob's partition: 0.9916999936103821\n",
      "Epoch 10/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 196/196 [00:31<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 2.2168004318159453, Accuracy on Carol's partition: 0.9929199814796448\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from secretflow import reveal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 使用 Cosine Similarity 计算损失\n",
    "def cosine_similarity(a, b):\n",
    "    # 计算余弦相似度\n",
    "    dot_product = tf.reduce_sum(a * b, axis=-1)  # 点积\n",
    "    norm_a = tf.norm(a, axis=-1)  # a 的范数\n",
    "    norm_b = tf.norm(b, axis=-1)  # b 的范数\n",
    "    cosine_sim = dot_product / (norm_a * norm_b + 1e-8)  # 计算余弦相似度，防止除以零\n",
    "    return cosine_sim\n",
    "\n",
    "# 定义单轮训练函数（手动计算损失和梯度）\n",
    "def train_one_epoch(partition_data, partition_labels, shared_model, previous_weights, optimizer=None, batch_size=128, mu = 2, temperature = 0.5):\n",
    "    # 从分区中提取数据和标签\n",
    "    data = reveal(partition_data)\n",
    "    labels = reveal(partition_labels)\n",
    "    \n",
    "    # 调整数据形状为模型的输入格式\n",
    "    data = data.reshape(-1, 28, 28, 1)  # 确保形状为 (样本数, 28, 28, 1)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = create_dense_model()\n",
    "    previous_models = [create_dense_model() for _ in previous_weights]\n",
    "\n",
    "\n",
    "    \n",
    "    # 初始化权重（如果提供了初始权重）\n",
    "    if shared_model.get_weights() is not None:\n",
    "        model.set_weights(shared_model.get_weights())\n",
    "    for previous_model, previous_weight in zip(previous_models, previous_weights):\n",
    "        if previous_weight is not None:  # 检查每个权重是否为 None\n",
    "            previous_model.set_weights(previous_weight)\n",
    "\n",
    "    # 使用优化器（默认是 Adam，如果没有传入）\n",
    "    if optimizer is None:\n",
    "        optimizer = tf.keras.optimizers.Adam()  # 默认使用 Adam 优化器\n",
    "    \n",
    "    # 手动计算损失\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(batch_size)  # 使用数据集和批次大小\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # 创建准确率计算指标\n",
    "\n",
    "    accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "    \n",
    "    for batch_data, batch_labels in tqdm(dataset, desc=\"Training Progress\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向传播\n",
    "            predictions = model(batch_data, training=True)\n",
    "            predictions_shared = shared_model(batch_data, training=True)\n",
    "            predictions_previous_models = [model(batch_data, training=True) for model in previous_models]\n",
    "\n",
    "            # 计算余弦相似度并生成新的预测结果\n",
    "            new_predictions = cosine_similarity(predictions, predictions_shared)\n",
    "            new_predictions_previous = [cosine_similarity(predictions, predictions_previous_model) for predictions_previous_model in predictions_previous_models]\n",
    "\n",
    "            # 变形为 (batch_size, 1)\n",
    "            logits = tf.reshape(new_predictions, (-1, 1))\n",
    "            logits_previous = [tf.reshape(new_prediction_previous, (-1, 1)) for new_prediction_previous in new_predictions_previous]\n",
    "            logits_previous_cat = tf.concat(logits_previous, axis=1)\n",
    "            logits = tf.concat([logits, logits_previous_cat], axis=1)\n",
    "            logits /= temperature\n",
    "\n",
    "            # 计算损失\n",
    "            loss1 = tf.keras.losses.sparse_categorical_crossentropy(batch_labels, predictions, from_logits=False)\n",
    "            loss1 = tf.reduce_mean(loss1)  # 取平均值\n",
    "\n",
    "            # 计算对比损失 (第二部分)\n",
    "            # 假设使用与目标相同的标签，可以根据需要修改\n",
    "            labels = tf.zeros_like(batch_labels, dtype=tf.int64)\n",
    "            loss2 = mu * tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=False))\n",
    "\n",
    "            # 总损失 = 交叉熵损失 + 对比损失\n",
    "            loss = loss1 + loss2\n",
    "        \n",
    "        # 计算梯度\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # 应用梯度更新权重\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        epoch_loss += loss.numpy()  # 累积每批次的损失\n",
    "        \n",
    "        # 更新准确率\n",
    "        accuracy_metric.update_state(batch_labels, predictions)\n",
    "    \n",
    "    # 计算整个 epoch 的平均损失和准确率\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    avg_accuracy = accuracy_metric.result().numpy()  # 获取当前的准确率\n",
    "    accuracy_metric.reset_states()  # 重置准确率计算器\n",
    "    avg_gradients = [grad / len(dataset) for grad in accumulated_gradients]  # 平均每个梯度\n",
    "    \n",
    "    return model.get_weights(), avg_loss, avg_accuracy, avg_gradients  # 返回当前损失、准确率和更新后的权重\n",
    "\n",
    "# 外部控制 epochs 的循环\n",
    "num_epochs = 10\n",
    "weights_share = None\n",
    "weights_alice = None  # 初始权重为空\n",
    "weights_bob = None\n",
    "weights_carol = None\n",
    "previous_weights_alice = [None, None]\n",
    "previous_weights_bob = [None, None]\n",
    "previous_weights_carol = [None, None]\n",
    "\n",
    "# 假设 Alice 和 Bob 的数据量\n",
    "alice_data_size = len(alice_images)\n",
    "bob_data_size = len(bob_images)\n",
    "carol_data_size = len(carol_images)\n",
    "\n",
    "# 计算总数据量\n",
    "total_data_size = alice_data_size + bob_data_size + carol_data_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 更新共享模型权重的加权平均\n",
    "    if weights_alice is not None and weights_bob is not None and weights_carol is not None:\n",
    "        # 加权平均\n",
    "        weights_share = [\n",
    "            (wa * alice_data_size + wb * bob_data_size + wc * carol_data_size) / total_data_size\n",
    "            for wa, wb, wc in zip(weights_alice, weights_bob, weights_carol)\n",
    "        ]\n",
    "        # 设置共享模型的权重\n",
    "        shared_model.set_weights(weights_share)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Alice's partition...\")\n",
    "    weights_alice, loss_alice, acc_alice, avg_gradients_alice = train_one_epoch(alice_partition_images, alice_partition_labels, shared_model, previous_weights_alice)\n",
    "    print(f\"Loss on Alice's partition: {loss_alice}, Accuracy on Alice's partition: {acc_alice}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Bob's partition...\")\n",
    "    weights_bob, loss_bob, acc_bob, avg_gradients_bob = train_one_epoch(bob_partition_images, bob_partition_labels, shared_model, previous_weights_bob)\n",
    "    print(f\"Loss on Bob's partition: {loss_bob}, Accuracy on Bob's partition: {acc_bob}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Carol's partition...\")\n",
    "    weights_carol, loss_carol, acc_carol, avg_gradients_carol = train_one_epoch(carol_partition_images, carol_partition_labels, shared_model, previous_weights_carol)\n",
    "    print(f\"Loss on Carol's partition: {loss_carol}, Accuracy on Carol's partition: {acc_carol}\")\n",
    "\n",
    "    # 保存当前权重作为下一轮的 \"previous_weights\"\n",
    "    previous_weights_alice = [weights_bob, weights_carol]\n",
    "    previous_weights_bob = [weights_alice, weights_carol]\n",
    "    previous_weights_carol= [weights_alice, weights_bob]\n",
    "\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
