{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-26 10:08:54,973\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "INFO:root:Try init sf in SIMULATION mode\n",
      "/usr/local/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-12-26 10:08:58,063\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.00gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-12-26 10:08:58,197\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import secretflow as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sf.init(['alice', 'bob', 'carol'], address='local')\n",
    "alice, bob, carol = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('carol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pickle\n",
    "\n",
    "# CIFAR-100 下载 URL\n",
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "dataset_dir = './cifar100_data'  # 保存数据的目录\n",
    "filename = 'cifar-100-python.tar.gz'\n",
    "\n",
    "# 下载文件（如果尚未下载）\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "file_path = os.path.join(dataset_dir, filename)\n",
    "\n",
    "# 如果文件不存在，下载文件\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading CIFAR-100 dataset...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Download completed.\")\n",
    "\n",
    "# 解压文件\n",
    "if not os.path.exists(os.path.join(dataset_dir, 'cifar-100-python')):\n",
    "    print(\"Extracting CIFAR-100 dataset...\")\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=dataset_dir)\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "# 加载 CIFAR-100 数据\n",
    "def load_cifar100_data(dataset_dir):\n",
    "    \"\"\"加载 CIFAR-100 数据集\"\"\"\n",
    "    # 加载训练数据\n",
    "    train_batch_filename = os.path.join(dataset_dir, 'cifar-100-python', 'train')\n",
    "    with open(train_batch_filename, 'rb') as f:\n",
    "        train_data = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    # 提取训练数据\n",
    "    images_train = train_data['data']\n",
    "    labels_train = train_data['fine_labels']\n",
    "    \n",
    "    # 转换图片数据形状 (N, 3, 32, 32)\n",
    "    images_train = images_train.reshape(-1, 3, 32, 32).astype(np.uint8)\n",
    "\n",
    "    # 加载测试数据\n",
    "    test_batch_filename = os.path.join(dataset_dir, 'cifar-100-python', 'test')\n",
    "    with open(test_batch_filename, 'rb') as f:\n",
    "        test_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    # 提取测试数据\n",
    "    images_test = test_data['data']\n",
    "    labels_test = test_data['fine_labels']\n",
    "    \n",
    "    # 转换图片数据形状 (N, 3, 32, 32)\n",
    "    images_test = images_test.reshape(-1, 3, 32, 32).astype(np.uint8)\n",
    "    \n",
    "    return (images_train, labels_train), (images_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(image, label), (_, _) = load_cifar100_data(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_images = image[:15000]\n",
    "bob_images = image[15000:35000]\n",
    "carol_images = image[35000:]\n",
    "\n",
    "alice_labels = np.array(label[:15000])\n",
    "bob_labels = np.array(label[15000:35000])\n",
    "carol_labels = np.array(label[35000:])\n",
    "\n",
    "alice_partition_images = alice(lambda x: x)(alice_images)\n",
    "bob_partition_images = bob(lambda x: x)(bob_images)\n",
    "carol_partition_images = carol(lambda x: x)(carol_images)\n",
    "\n",
    "alice_partition_labels = alice(lambda x: x)(alice_labels)\n",
    "bob_partition_labels = bob(lambda x: x)(bob_labels)\n",
    "carol_partition_labels = carol(lambda x: x)(carol_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image partitions shape: {PYURuntime(alice): (15000, 3, 32, 32), PYURuntime(bob): (20000, 3, 32, 32), PYURuntime(carol): (15000, 3, 32, 32)}\n",
      "Label partitions shape: {PYURuntime(alice): (15000,), PYURuntime(bob): (20000,), PYURuntime(carol): (15000,)}\n"
     ]
    }
   ],
   "source": [
    "# 创建联邦数据集 FedNdarray\n",
    "from secretflow.data.ndarray import FedNdarray, PartitionWay\n",
    "\n",
    "# 图像数据\n",
    "federated_images = FedNdarray(\n",
    "    partitions={alice: alice_partition_images, bob: bob_partition_images, carol: carol_partition_images},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# 标签数据\n",
    "federated_labels = FedNdarray(\n",
    "    partitions={alice: alice_partition_labels, bob: bob_partition_labels, carol: carol_partition_labels},\n",
    "    partition_way=PartitionWay.HORIZONTAL,  # 水平分片\n",
    ")\n",
    "\n",
    "# # 检查分区信息\n",
    "print(\"Image partitions shape:\", federated_images.partition_shape())\n",
    "print(\"Label partitions shape:\", federated_labels.partition_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 10:09:03.560499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_dense_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(32, 32, 3)),  # 输入形状为 32*32 的 RGB 图像\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 1\n",
    "            layers.BatchNormalization(),  # 批归一化\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层 1\n",
    "            \n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 2\n",
    "            layers.BatchNormalization(),  # 批归一化\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层 2\n",
    "            \n",
    "            layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),  # 卷积层 3\n",
    "            layers.BatchNormalization(),  # 批归一化\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),  # 最大池化层 3\n",
    "            \n",
    "            layers.Flatten(),  # 拉平为一维向量\n",
    "            layers.Dense(512, activation=\"relu\"),  # 全连接层 1\n",
    "            layers.Dropout(0.5),  # Dropout 层，防止过拟合\n",
    "            layers.Dense(100, activation=\"softmax\")  # 输出层，100 个类别\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 在外部创建共享模型\n",
    "shared_model = create_dense_model()\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer.build(shared_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:27<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 6.272179223723331, Accuracy on Alice's partition: 0.026266666129231453\n",
      "Epoch 1/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:36<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 6.290257818379979, Accuracy on Bob's partition: 0.035100001841783524\n",
      "Epoch 1/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:27<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 6.284635782241821, Accuracy on Carol's partition: 0.029200000688433647\n",
      "Epoch 2/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:27<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 6.119342997922736, Accuracy on Alice's partition: 0.08166666328907013\n",
      "Epoch 2/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:36<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 6.044938521780026, Accuracy on Bob's partition: 0.08844999969005585\n",
      "Epoch 2/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:27<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 6.113766953096551, Accuracy on Carol's partition: 0.07199999690055847\n",
      "Epoch 3/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:27<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Alice's partition: 5.874483173176394, Accuracy on Alice's partition: 0.125533327460289\n",
      "Epoch 3/10 on Bob's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 157/157 [00:36<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Bob's partition: 5.818306850020293, Accuracy on Bob's partition: 0.13375000655651093\n",
      "Epoch 3/10 on Carol's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 118/118 [00:28<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Carol's partition: 5.885057291742099, Accuracy on Carol's partition: 0.12826666235923767\n",
      "Epoch 4/10 on Alice's partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  57%|█████▋    | 67/118 [00:15<00:12,  4.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10693/3342828271.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# 设置共享模型的权重\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mshared_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_share\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs} on Alice's partition...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mweights_alice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_alice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_alice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_gradients_alice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malice_partition_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malice_partition_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_weights_alice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss on Alice's partition: {loss_alice}, Accuracy on Alice's partition: {acc_alice}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs} on Bob's partition...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10693/3342828271.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(partition_data, partition_labels, shared_model, previous_weights, optimizer, batch_size, mu, temperature)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# 总损失 = 交叉熵损失 + 对比损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# 计算梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# 应用梯度更新权重\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1059\u001b[0m               output_gradients))\n\u001b[1;32m   1060\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    588\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m           data_format=data_format),\n\u001b[0;32m--> 592\u001b[0;31m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[1;32m    593\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         data_format, \"dilations\", dilations)\n\u001b[1;32m   1260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from secretflow import reveal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 使用 Cosine Similarity 计算损失\n",
    "def cosine_similarity(a, b):\n",
    "    # 计算余弦相似度\n",
    "    dot_product = tf.reduce_sum(a * b, axis=-1)  # 点积\n",
    "    norm_a = tf.norm(a, axis=-1)  # a 的范数\n",
    "    norm_b = tf.norm(b, axis=-1)  # b 的范数\n",
    "    cosine_sim = dot_product / (norm_a * norm_b + 1e-8)  # 计算余弦相似度，防止除以零\n",
    "    return cosine_sim\n",
    "\n",
    "# 定义单轮训练函数（手动计算损失和梯度）\n",
    "def train_one_epoch(partition_data, partition_labels, shared_model, previous_weights, optimizer=None, batch_size=128, mu = 2, temperature = 0.5):\n",
    "    # 从分区中提取数据和标签\n",
    "    data = reveal(partition_data)\n",
    "    labels = reveal(partition_labels)\n",
    "    \n",
    "    # 调整数据形状为模型的输入格式\n",
    "    data = data.reshape(-1, 32, 32, 3)  # 确保形状为 (样本数, 32, 32, 3)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = create_dense_model()\n",
    "    previous_models = [create_dense_model() for _ in previous_weights]\n",
    "\n",
    "\n",
    "    \n",
    "    # 初始化权重（如果提供了初始权重）\n",
    "    if shared_model.get_weights() is not None:\n",
    "        model.set_weights(shared_model.get_weights())\n",
    "    for previous_model, previous_weight in zip(previous_models, previous_weights):\n",
    "        if previous_weight is not None:  # 检查每个权重是否为 None\n",
    "            previous_model.set_weights(previous_weight)\n",
    "\n",
    "    # 使用优化器（默认是 Adam，如果没有传入）\n",
    "    if optimizer is None:\n",
    "        optimizer = tf.keras.optimizers.Adam()  # 默认使用 Adam 优化器\n",
    "    \n",
    "    # 手动计算损失\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(batch_size)  # 使用数据集和批次大小\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # 创建准确率计算指标\n",
    "\n",
    "    accumulated_gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "    \n",
    "    for batch_data, batch_labels in tqdm(dataset, desc=\"Training Progress\"):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向传播\n",
    "            predictions = model(batch_data, training=True)\n",
    "            predictions_shared = shared_model(batch_data, training=True)\n",
    "            predictions_previous_models = [model(batch_data, training=True) for model in previous_models]\n",
    "\n",
    "            # 计算余弦相似度并生成新的预测结果\n",
    "            new_predictions = cosine_similarity(predictions, predictions_shared)\n",
    "            new_predictions_previous = [cosine_similarity(predictions, predictions_previous_model) for predictions_previous_model in predictions_previous_models]\n",
    "\n",
    "            # 变形为 (batch_size, 1)\n",
    "            logits = tf.reshape(new_predictions, (-1, 1))\n",
    "            logits_previous = [tf.reshape(new_prediction_previous, (-1, 1)) for new_prediction_previous in new_predictions_previous]\n",
    "            logits_previous_cat = tf.concat(logits_previous, axis=1)\n",
    "            logits = tf.concat([logits, logits_previous_cat], axis=1)\n",
    "            logits /= temperature\n",
    "\n",
    "            # 计算损失\n",
    "            loss1 = tf.keras.losses.sparse_categorical_crossentropy(batch_labels, predictions, from_logits=False)\n",
    "            loss1 = tf.reduce_mean(loss1)  # 取平均值\n",
    "\n",
    "            # 计算对比损失 (第二部分)\n",
    "            # 假设使用与目标相同的标签，可以根据需要修改\n",
    "            labels = tf.zeros_like(batch_labels, dtype=tf.int64)\n",
    "            loss2 = mu * tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=False))\n",
    "\n",
    "            # 总损失 = 交叉熵损失 + 对比损失\n",
    "            loss = loss1 + loss2\n",
    "        \n",
    "        # 计算梯度\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # 应用梯度更新权重\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        epoch_loss += loss.numpy()  # 累积每批次的损失\n",
    "        \n",
    "        # 更新准确率\n",
    "        accuracy_metric.update_state(batch_labels, predictions)\n",
    "    \n",
    "    # 计算整个 epoch 的平均损失和准确率\n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    avg_accuracy = accuracy_metric.result().numpy()  # 获取当前的准确率\n",
    "    accuracy_metric.reset_states()  # 重置准确率计算器\n",
    "    avg_gradients = [grad / len(dataset) for grad in accumulated_gradients]  # 平均每个梯度\n",
    "    \n",
    "    return model.get_weights(), avg_loss, avg_accuracy, avg_gradients  # 返回当前损失、准确率和更新后的权重\n",
    "\n",
    "# 外部控制 epochs 的循环\n",
    "num_epochs = 10\n",
    "weights_share = None\n",
    "weights_alice = None  # 初始权重为空\n",
    "weights_bob = None\n",
    "weights_carol = None\n",
    "previous_weights_alice = [None, None]\n",
    "previous_weights_bob = [None, None]\n",
    "previous_weights_carol = [None, None]\n",
    "\n",
    "# 假设 Alice 和 Bob 的数据量\n",
    "alice_data_size = len(alice_images)\n",
    "bob_data_size = len(bob_images)\n",
    "carol_data_size = len(carol_images)\n",
    "\n",
    "# 计算总数据量\n",
    "total_data_size = alice_data_size + bob_data_size + carol_data_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 更新共享模型权重的加权平均\n",
    "    if weights_alice is not None and weights_bob is not None and weights_carol is not None:\n",
    "        # 加权平均\n",
    "        weights_share = [\n",
    "            (wa * alice_data_size + wb * bob_data_size + wc * carol_data_size) / total_data_size\n",
    "            for wa, wb, wc in zip(weights_alice, weights_bob, weights_carol)\n",
    "        ]\n",
    "        # 设置共享模型的权重\n",
    "        shared_model.set_weights(weights_share)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Alice's partition...\")\n",
    "    weights_alice, loss_alice, acc_alice, avg_gradients_alice = train_one_epoch(alice_partition_images, alice_partition_labels, shared_model, previous_weights_alice)\n",
    "    print(f\"Loss on Alice's partition: {loss_alice}, Accuracy on Alice's partition: {acc_alice}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Bob's partition...\")\n",
    "    weights_bob, loss_bob, acc_bob, avg_gradients_bob = train_one_epoch(bob_partition_images, bob_partition_labels, shared_model, previous_weights_bob)\n",
    "    print(f\"Loss on Bob's partition: {loss_bob}, Accuracy on Bob's partition: {acc_bob}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} on Carol's partition...\")\n",
    "    weights_carol, loss_carol, acc_carol, avg_gradients_carol = train_one_epoch(carol_partition_images, carol_partition_labels, shared_model, previous_weights_carol)\n",
    "    print(f\"Loss on Carol's partition: {loss_carol}, Accuracy on Carol's partition: {acc_carol}\")\n",
    "\n",
    "    # 保存当前权重作为下一轮的 \"previous_weights\"\n",
    "    previous_weights_alice = [weights_bob, weights_carol]\n",
    "    previous_weights_bob = [weights_alice, weights_carol]\n",
    "    previous_weights_carol= [weights_alice, weights_bob]\n",
    "\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
